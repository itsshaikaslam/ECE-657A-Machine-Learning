{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PART1_Documentation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMwbtohE8iePUeVfCubkkBm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8HDF1P5L2IaV","colab_type":"text"},"source":["### **Question 1: Classification: Feature Extraction + Classical Methods**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VfJitY_k2n4-","colab_type":"text"},"source":["### **Feature Extraction**"]},{"cell_type":"markdown","metadata":{"id":"WofVYVb62lEF","colab_type":"text"},"source":["Feature extraction techniques helps in accuracy improvements, overfitting risk reduction, speed up in training, improve Data Visualization."]},{"cell_type":"markdown","metadata":{"id":"egzCUWV5BnAW","colab_type":"text"},"source":["**Experiments with PCA:**\n"]},{"cell_type":"markdown","metadata":{"id":"nfKG30BkDRPF","colab_type":"text"},"source":["For the given Fashion MNIST data the first two components of PCA explains about 46.75 %  of the total variance.\n","\n","When plotted how much weight each pixel in the clothing picture gets in the resulting vector, using a heatmap. This created a interpretable picture of what each vector is \"finding\".\n","\n","![alt text](https://drive.google.com/uc?id=1csMQPs7b5DGGipo-x9HqoJZ8C74bAXQp)\n","\n","The first component looks like...some kind of large clothing object (e.g. not a shoe or accessor) something like a shirt. The second component looks like negative space around a pair of pants."]},{"cell_type":"markdown","metadata":{"id":"o7Jv5xNAF94u","colab_type":"text"},"source":["When using PCA, it's a good idea to pick a decomposition with a reasonably small number of variables by looking for a \"cutoff\" in the effectiveness of the model. \n","\n","From the below plot for cumilative explained variance ratio and  for explained variance ratio for each of the components of the PCA, we can observe that first components of PCA contribute to the most of the variance.\n","\n","We can see that only 100 PCA components can explain more than 90% variance.Selecting just 100 components among 784 decreases the size of data largely and can result in less training time.We will look at the results of the training time in coming discussion and how it helps in avoiding overfitting the model.\n","\n","\n","**code for plotting explaned variace graph**\n","```\n","pca = PCA().fit(data)\n","plt.figure(figsize=(15,10))\n","plt.plot(pca.explained_variance_ratio_,color='navy',label='explained_variance_ratio of each component')\n","plt.plot(np.cumsum(pca.explained_variance_ratio_),color='darkorange',label='cumulative explained_variance_ratio')\n","plt.title(\"Component-wise and Cumulative Explained Variance\")\n","plt.legend(loc=\"center right\",prop={'size': 14})\n","plt.grid()\n","plt.show()\n","```\n","\n","\n","**explaned variace graph**\n","![alt text](https://drive.google.com/uc?id=1Jx1x-Daqq3VtIpVbVAdt7L1KE_ieUGE9)"]},{"cell_type":"markdown","metadata":{"id":"KGp-z8CuF-GQ","colab_type":"text"},"source":["**Visualising first two components of PCA**\n","\n","\n","```\n","def scatter_plot(x, colors,method):\n","    # choose a color palette with seaborn.\n","    num_classes = len(np.unique(colors))\n","    palette = np.array(sns.color_palette(\"hls\", num_classes))\n","\n","    # create a scatter plot.\n","    f = plt.figure(figsize=(8, 8))\n","    ax = plt.subplot(aspect='equal')\n","    plt.xlabel(\"PCA 1\")\n","    plt.ylabel(\"PCA 2\")\n","    sc = ax.scatter(x[:,0], x[:,1], c=palette[colors.astype(np.int)])\n","    ax.set_title(method)\n","\n","scatter_plot(data_pca[:,0:2], labels,\"kernel PCA\")\n","```\n","\n","\n","\n","![alt text](https://drive.google.com/uc?id=1Wg66RFJFupr-moOK-9D_K06ZFx2Iozbk)"]},{"cell_type":"markdown","metadata":{"id":"nE77z0XuF-Ib","colab_type":"text"},"source":["**Visualization with LDA:**\n","\n","\n","![alt text](https://drive.google.com/uc?id=1vHBgF57BTuVoFMrwIaucvALavf9ttsRh)"]},{"cell_type":"markdown","metadata":{"id":"dGXADIVWF-L-","colab_type":"text"},"source":["## **Exploring Classic ML models**"]},{"cell_type":"markdown","metadata":{"id":"nA9isrNwTrCW","colab_type":"text"},"source":["### **KNN**"]},{"cell_type":"markdown","metadata":{"id":"1Re-tr8tF-OW","colab_type":"text"},"source":["First checking how a simple algorithm like **KNN** is performing on the dataset "]},{"cell_type":"markdown","metadata":{"id":"r-QuXzuFF-Qr","colab_type":"text"},"source":["We can check how much time KNN will be taking on a subset of samples(10k) with all features vs considering first 120 components of PCA."]},{"cell_type":"markdown","metadata":{"id":"HKaqtS9W9z5h","colab_type":"text"},"source":["\n","![alt text](https://drive.google.com/uc?id=1DKT7h5b7DpWX-jlnCiO8ysa-ntYhDFdC)\n","\n","*   Cross Validation mean accuracy score for C= 13 with all features is 0.7843749 where as it is 0.8166249 with 120 PCA components\n","*   time taken for training KNN for all features is 0.75564956 sec whereas it is just 0.1107163 sec\n","*  test set accuracy score for C= 13 with all features is 0.7955 and with 120 PCA components is 0.8185\n","*    time taken for predicting KNN for all features is 31.26071 sec and with 1.63198 sec\n","\n","\n","*We can notice that with about only 120 PCA components , KNN's accuracy for crass validation and test is better than considering all features along with less time taken for both training and predicting.As said above feature extraction helped in improving each and every aspect of KNN in this case.*\n","\n","\n","So we can now start tuning KNN for different number of PCA components and with different parameters to achieve good accuracy.\n"]},{"cell_type":"markdown","metadata":{"id":"75Ec7nzZFJS5","colab_type":"text"},"source":["**Using Grid Search to find the best parameters**"]},{"cell_type":"markdown","metadata":{"id":"leYlxbghFU3x","colab_type":"text"},"source":["\n","\n","```\n","# defining the parameter values that should be searched\n","k_range = list(range(11,71,15))\n","weight_options = ['uniform', 'distance']\n","# intialising search grid\n","param_grid = dict(n_neighbors=k_range, weights=weight_options)\n","print(param_grid)\n","# splitting data\n","train_x,test_x,train_y,test_y=train_test_split(data_pca_subset[0:30000],labels[0:30000],test_size=0.2,random_state=42)\n","\n","grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\n","grid.fit(train_x, train_y)\n","\n","print(grid.best_estimator_)\n","print(grid.best_score_)\n","print(grid.best_params_)\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LfUWzwFOF03N","colab_type":"text"},"source":["For 30,000 samples :  obtained best parameters and its accuracy are :\n","\n","**best parameters : {'n_neighbors': 11, 'weights': 'distance'}**\n","\n","**Accuracy : 0.8476250000000001**\n","\n","So lets train KNN for whole dataset with 120 PCA components and test the accuracy with a small test set."]},{"cell_type":"markdown","metadata":{"id":"6L3j4teVgvNn","colab_type":"text"},"source":["```\n","train_x,test_x,train_y,test_y=train_test_split(data_pca_subset,labels,test_size=0.1,random_state=42)\n","\n","time_start = time.time()\n","knn = KNeighborsClassifier(n_neighbors=11,weights='distance')\n","knn.fit(train_x, train_y)\n","time_end = time.time()\n","scores=cross_val_score(knn, train_x ,train_y, cv=10) \n","\n","print(\"Validation set mean accuracy score for C= {0} with 120 PCA components is {1}\".format(11,scores.mean()))\n","print(\"time taken for training KNN with 120 PCA components {0}\".format(time_end -time_start))\n","\n","#testing how much time KNN will take the predict the output\n","time_start = time.time()\n","y_pred = knn.predict(test_x)\n","time_end = time.time()\n","accscore = accuracy_score(test_y, y_pred)\n","print(\"test set accuracy score for C= {0} with 120 PCA components is {1}\".format(13,accscore))\n","print(\"time taken for predicting output with 120 PCA components {0}\".format(time_end -time_start))\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SSBrEWatF1Ce","colab_type":"text"},"source":["\n","\n","*   Validation set mean accuracy score for C= 11 with 120 PCA components is **0.8617**\n","*   Time taken for training KNN with 120 PCA components **1.1164 sec**\n","* Test set accuracy score for C= 11 with 120 PCA components is **0.863**\n","* Time taken for predicting output with 120 PCA components **24.1985 sec** \n","\n","\n","\n","KNN is a Lazy learning algorithm.Lazy learning is a learning method in which generalization of the training data is,  delayed until a query is made to the system.So to predict the results KNN has to caluclate distances for multiple data points.As a result predicting results in KNN take huge time when compared to training.\n","\n","If predicting quickly is our application,then KNN is not our guy.\n","\n","**KNN ROC : ROC graph for different class of KNN along with a macro-average curve.**\n","\n","![alt text](https://drive.google.com/uc?id=1G8IdkM7UxFxCe3og1S0JTBgtWYsoJ7Hp)\n","\n","\n","\n","\n","\n","*   We can see that AuC under macro-average curve is .98.\n","*   The classification performed by the model is less than average for 'class 1' and 'class 2'\n","*   This model seems to classify 'class 0' and 'class 4' to good extent when compared to other class.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eSSBGfIGF1EP","colab_type":"text"},"source":["Lets train KNN with all data sample without any training set and then predict the labels for test set and then submit the predictions in Kaggle.\n","\n","* **time taken for training KNN with 120 PCA components 0.972000 sec**\n","* **time taken for prediction using KNN with 120 PCA components 59.21179 sec**\n","* **Kaagle Submission Accuracy :0.85780**\n","\n","Lets try bit more complex model like Random Forest"]},{"cell_type":"markdown","metadata":{"id":"9WEYMc34Y5eB","colab_type":"text"},"source":["## **Random Forest**"]},{"cell_type":"markdown","metadata":{"id":"_Mx6z3KcZfHQ","colab_type":"text"},"source":["Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that work by building a multitude of decision trees at training time and outputting the class which is the class mode (classification) or mean prediction (regression) of the individual trees."]},{"cell_type":"markdown","metadata":{"id":"0AuTmb7JZfM0","colab_type":"text"},"source":["Grid search Results for Random Forest for 5000 samples is as shown below\n","\n","\n","```\n","Mean Accuray score when depth=20 and num of tress=50 is : 0.762\n","Mean Accuray score when depth=20 and num of tress=150 is : 0.786\n","Mean Accuray score when depth=20 and num of tress=200 is : 0.7830000000000001\n","Mean Accuray score when depth=20 and num of tress=300 is : 0.7847500000000001\n","Mean Accuray score when depth=30 and num of tress=50 is : 0.766\n","Mean Accuray score when depth=30 and num of tress=150 is : 0.782\n","Mean Accuray score when depth=30 and num of tress=200 is : 0.7855\n","Mean Accuray score when depth=30 and num of tress=300 is : 0.7845\n","Mean Accuray score when depth=50 and num of tress=50 is : 0.7660000000000001\n","Mean Accuray score when depth=50 and num of tress=150 is : 0.7825\n","Mean Accuray score when depth=50 and num of tress=200 is : 0.786\n","Mean Accuray score when depth=50 and num of tress=300 is : 0.78375\n","Mean Accuray score when depth=60 and num of tress=50 is : 0.7660000000000001\n","Mean Accuray score when depth=60 and num of tress=150 is : 0.7825\n","Mean Accuray score when depth=60 and num of tress=200 is : 0.786\n","Mean Accuray score when depth=60 and num of tress=300 is : 0.78175\n","Mean Accuray score when depth=None and num of tress=50 is : 0.7660000000000001\n","Mean Accuray score when depth=None and num of tress=150 is : 0.7825\n","Mean Accuray score when depth=None and num of tress=200 is : 0.786\n","Mean Accuray score when depth=None and num of tress=300 is : 0.78075\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_QwOqwJsZfPf","colab_type":"text"},"source":["We can observer that for **depth =20 and trees = 150** ,mean validation accuracy seems to be good.\n","\n","Lets analysie it using ROC curve for (depth =20,trees = 10) ,(depth =20,trees = 20) and (depth =20,trees = 150) \n","\n","Considering the extension of ROC for multiclass variables by considering macro-average of the ROC curves of each class.\n","\n","**Reason to consider Macro-average instead of micro-average:** Micro- and macro-averages will calculate slightly different things, and thus differ in their interpretation. A macro-average calculates the metric for each class independently and then takes the average (thus treating all classes equally) whereas a micro-average aggregates the contributions of all classes to calculate the average metric. In a multi-class classification setup, micro-average is preferable if you suspect there might be class imbalance.\n","\n","In Fashion-MNIST dataset all the classes have equal and no imbalance is obsserved. So choosing macro-average over micro-average.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MOiXFlnB7Tue","colab_type":"text"},"source":["Sample code for generating macro- average ROC for each model: \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Pgp1uHtk9Dg9","colab_type":"text"},"source":["\n","\n","```\n","classifier = OneVsRestClassifier(RandomForestClassifier(max_depth=20, random_state=42,n_estimators=10))\n","y_score = classifier.fit(X_train, y_train).predict_proba(X_test)\n","\n","# Compute ROC curve and ROC area for each class\n","fpr_rfc_10 = dict()\n","tpr_rfc_10 = dict()\n","roc_auc_rfc_10 = dict()\n","for i in range(n_classes):\n","    fpr_rfc_10[i], tpr_rfc_10[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n","    roc_auc_rfc_10[i] = auc(fpr_rfc_10[i], tpr_rfc_10[i])\n","\n","# Compute micro-average ROC curve and ROC area\n","fpr_rfc_10[\"micro\"], tpr_rfc_10[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n","roc_auc_rfc_10[\"micro\"] = auc(fpr_rfc_10[\"micro\"], tpr_rfc_10[\"micro\"])\n","\n","# Compute ROC curve and ROC area for each class\n","fpr_rfc_10 = dict()\n","tpr_rfc_10 = dict()\n","roc_auc_rfc_10 = dict()\n","for i in range(n_classes):\n","    fpr_rfc_10[i], tpr_rfc_10[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n","    roc_auc_rfc_10[i] = auc(fpr_rfc_10[i], tpr_rfc_10[i])\n","\n","# Compute micro-average ROC curve and ROC area\n","fpr_rfc_10[\"micro\"], tpr_rfc_10[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n","roc_auc_rfc_10[\"micro\"] = auc(fpr_rfc_10[\"micro\"], tpr_rfc_10[\"micro\"])\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JaGJxsMk9azH","colab_type":"text"},"source":["Plotting ROC graph\n","\n","\n","\n","```\n","# Ploting only macro ROC curves\n","plt.figure(figsize=(10,8))\n","\n","plt.plot(fpr_rfc_10[\"macro\"], tpr_rfc_10[\"macro\"],\n","         label='ROC curve (area = {0:0.2f}) RFC n_components = 10'\n","               ''.format(roc_auc_rfc_10[\"macro\"]),\n","         color='green', linestyle=':', linewidth=4)\n","\n","plt.plot(fpr_rfc_50[\"macro\"], tpr_rfc_50[\"macro\"],\n","         label=' ROC curve (area = {0:0.2f}) RFC n_components = 50'\n","               ''.format(roc_auc_rfc_50[\"macro\"]),\n","         color='navy', linestyle=':', linewidth=4)\n","\n","plt.plot(fpr_rfc_150[\"macro\"], tpr_rfc_150[\"macro\"],\n","         label='ROC curve (area = {0:0.2f}) RFC n_components = 150'\n","               ''.format(roc_auc_rfc_150[\"macro\"]),\n","         color='red', linestyle=':', linewidth=4)\n","\n","lw = 2\n","plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('macro-average Receiver operating characteristic of multi-class RFC for different number of estimators')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yywxCeyJZfSY","colab_type":"text"},"source":["![alt text](https://drive.google.com/uc?id=1jwbKPzwc7ZEfjiYiQohpjohqqCMDhwJT)"]},{"cell_type":"markdown","metadata":{"id":"2mXUFun7ZfUn","colab_type":"text"},"source":["We can observe that by changing the number of components for RFC the AUC under ROC's is changing accordingly.\n","\n","From the graph we can see that AUC for 150 components is greater than the remaining two ,incidating high true positive probability.So we can select the parameters for which AUC is more.\n","\n","So , selecting (depth =20,trees = 150) as the best set of parameters.\n","\n","Now training RFC with 120 PCA components for all samples and submitting in kaggle.\n","\n","* **time taken for training RFC with 120 PCA components 122.430294 sec**\n","* **time taken for prediction using RFC with 120 PCA components 0.3924 sec**\n","* **Kaagle Submission Accuracy :0.84980**\n","\n","Now training RFC with 40 PCA components for all samples and submitting in kaggle.\n","\n","* **time taken for training RFC with 40 PCA components 67.3108 sec**\n","* **time taken for prediction using RFC with 40 PCA components  0.3455 sec**\n","* **Kaagle Submission Accuracy :0.87540**\n","\n","When considering 120 components the model is getting overfit as we can see that the accuracy is dropping as we use more components.\n","\n","We can see that the prediction time for for RFC is very less compared to its tarining time. Which is inverse of KNN."]},{"cell_type":"markdown","metadata":{"id":"MHOErEcYZfXb","colab_type":"text"},"source":["## **SVM**"]},{"cell_type":"markdown","metadata":{"id":"OTJmHH0yZfZg","colab_type":"text"},"source":["\n","\n","*   Support-vector machines are supervised learning models with associated learning algorithms which analyse the data used for classification and regression analysis.\n","*   An SVM training algorithm builds a model that assigns new examples to one or the other category, given a set of training examples, each marked as belonging to one or the other category.\n","*   An SVM model is a representation of the examples as space points, mapped in such a way that the samples of the separate categories are divided by a clear gap as large as possible.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zWTcVO7XxRaw","colab_type":"text"},"source":["**Design Choices :**\n","\n","**Kernel:** Let us make a trail to check which kernel is performing better with 5k samples.\n","\n","* Checking performance for c_Values = [0.1,1,10,20] for 5k samples and measuring the time taken for finishing scross validation.\n","\n","### **Kernel = rbf**\n","\n","\n","**sample code**\n","```\n","c_Values = [0.1,1,10,20]\n","\n","data_pca_subset = data_pca[:,0:40]\n","\n","time_start = time.time()\n","for x in c_Values :\n","    rbf_svc = svm.SVC(kernel='rbf',C=x)\n","    scores=cross_val_score(rbf_svc, data_pca_subset[0:5000], labels[0:5000], cv=10)\n","    print(\"Kernal = rbf and Validation set mean accuracy score for C= {0} is {1}\".format(x,scores.mean()))\n","time_end = time.time()\n","print(\"time taken for getting cross validation score for rbf SVM with 40 PCA components {0}\".format(time_end -time_start))\n","```\n","\n","**obtained rsults**\n","\n","![alt text](https://drive.google.com/uc?id=12aKx7dP0L7v8jSLj6CttlDcpev_4zl4y)\n","\n","\n","### **Kernel = linear**\n","\n","\n","**sample code**\n","```\n","c_Values = [0.1,1,10,20]\n","\n","data_pca_subset = data_pca[:,0:40]\n","time_start = time.time()\n","for x in c_Values :\n","    lin_svc = svm.SVC(kernel='linear',C=x)\n","    scores=cross_val_score(lin_svc, data_pca_subset[0:5000], labels[0:5000], cv=10)\n","    print(\"Kernal = rbf and Validation set mean accuracy score for C= {0} is {1}\".format(x,scores.mean()))\n","time_end = time.time()\n","print(\"time taken for getting cross validation score for linear SVM with 40 PCA components {0}\".format(time_end -time_start))\n","```\n","\n","**obtained results**\n","\n","![alt text](https://drive.google.com/uc?id=1_RB8hizRxCVi-e0s_-o3vb1BTjJAMFMV)\n","\n","**observations:**\n","*   We can see that rbf kernel's performance is better than linear for the samle data both in terms accuracy and time taken for cross validation.\n","\n","**So, using rbf kernel.**\n"]},{"cell_type":"markdown","metadata":{"id":"7wuvBOvli3wo","colab_type":"text"},"source":["### **Trying to tune the parameters :**\n","\n","Results obtained for various C values and its mean cross validation accuracy s as shown below:\n","\n","\n","**sample code**\n","```\n","c_Values = [0.1,1,10,20,40,60,100,120]\n","\n","for x in c_Values :\n","    lin_svc = svm.SVC(kernel='rbf',C=x)\n","    scores=cross_val_score(lin_svc, data_pca_subset[0:20000], labels[0:20000], cv=10)\n","    print(\"Validation set mean accuracy score for C= {0} is {1}\".format(x,scores.mean()))\n","```\n","\n","\n","**obtained results**\n","```\n","Validation set mean accuracy score for C= 0.1 is 0.7952999999999999\n","Validation set mean accuracy score for C= 1 is 0.85185\n","Validation set mean accuracy score for C= 10 is 0.8676999999999999\n","Validation set mean accuracy score for C= 20 is 0.8693500000000001\n","Validation set mean accuracy score for C= 40 is 0.86295\n","Validation set mean accuracy score for C= 60 is 0.86\n","Validation set mean accuracy score for C= 100 is 0.8564499999999999\n","Validation set mean accuracy score for C= 120 is 0.8551\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TEuMKW8ki30T","colab_type":"text"},"source":["**Now training SVM with C = 20 with 120 PCA components for all samples and testing the accuracy in kaagle.**\n","\n","\n","\n","*   time taken for training SVM with 120 PCA components **329.72028 sec**\n","*   time taken for predicting output with RFC and 120 PCA components **36.33985 sec**\n","*   Kaagle Submission Accuracy :**0.88720**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dD_UwXDLi33l","colab_type":"text"},"source":["**Now training SVM with C = 20 with 50 PCA components for all samples and testing the accuracy in kaagle.**\n","\n","*   time taken for training SVM with 50 PCA components **119.66347 sec**\n","*   time taken for predicting output with SVM and 50 PCA components **17.000176 sec**\n","*   Kaagle Submission Accuracy :**0.89500**\n","\n","\n","\n","**sample code**\n","```\n","time_start = time.time()\n","lin_svc = svm.SVC(kernel='rbf',C=20).fit(pca_data_new,labels)\n","time_end = time.time()\n","print(\"time taken for training SVM with 50 PCA components {0}\".format(time_end -time_start))\n","\n","time_start = time.time()\n","predicted = lin_svc.predict(test_pca_new)\n","time_end = time.time()\n","print(predicted.shape)\n","print(\"time taken for predicting output with SVM and 50 PCA components {0}\".format(time_end -time_start))\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"oAH_TWEbi36o","colab_type":"text"},"source":["**Now training SVM with C = 20 with 35 PCA components for all samples and testing the accuracy in kaagle.**\n","\n","*   time taken for training SVM with 35 PCA components **88.02530288 sec**\n","*   time taken for predicting output with SVM and 35 PCA components **12.8491978 sec**\n","*   Kaagle SUbmission Accuracy :**0.89200**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Sr6ZzArw17Ul","colab_type":"text"},"source":["**Now training SVM with C = 20 with all PCA components for all samples and testing the accuracy in kaagle.**\n","\n","*   time taken for training SVM with all PCA components **3438.19957 sec**\n","*   time taken for predicting output with SVM and all PCA components **319.45426**\n","*   Kaagle SUbmission Accuracy :**0.88480**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7nWURnclHYG3","colab_type":"text"},"source":["### **Comparing results:**\n","\n","![alt text](https://drive.google.com/uc?id=1iz8z2POE1PrI8ZIs-vGAQt-J_NWei_mW)"]},{"cell_type":"markdown","metadata":{"id":"gIL6KANhFY7E","colab_type":"text"},"source":["### **Conclusions:**\n","\n","***From the above results we can notice that as we use more number of PCA components the model is getting overfit and the accuracy is getting decreased.***\n","\n","***With less PCA components both training the model and prediction is quite fast without compramising much on the accuracy.***\n","\n","**So choosing minimal PCA components can avoid overfitting the model.** "]},{"cell_type":"markdown","metadata":{"id":"-BDTKFGQgL2B","colab_type":"text"},"source":["### **Trying XGBoost**"]},{"cell_type":"markdown","metadata":{"id":"9IYxbHKDTQK6","colab_type":"text"},"source":["***Understanding XGBoost***\n","\n","\n","\n","*   With a regular machine learning model, we would simply train a single model on our dataset, like a decision tree, and use that for prediction.\n","*   On the other hand, boosting is taking a more iterative approach. It is still technically an ensemble technique that combines many models to perform the final one together, but takes a cleverer approach.\n","*   Instead of training all models in isolation from each other, train models are boosted successively, with each new model being trained to correct previous errors.\n","*   XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework.\n","*   XGBoost improves upon the base GBM framework through systems optimization and algorithmic enhancements. \n","\n","\n","Ref : [XGBoost Algorithm: Long May She Reign!](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)\n","\n","\n","Ref : [Exploring XGBoost](https://towardsdatascience.com/exploring-xgboost-4baf9ace0cf6)\n"]},{"cell_type":"markdown","metadata":{"id":"toh0XeuNgTMV","colab_type":"text"},"source":["**Tried to implement XGBoost with below parameters :**\n","\n","**Choosen an early stopping = 50 to not overfit the model.**\n","\n","\n","**sample code depth = 20 and n_classes = 10**\n","```\n","param_list = [(\"eta\", 0.08),  (\"max_depth\",20),(\"subsample\", 0.8), (\"colsample_bytree\", 0.8), (\"objective\", \"multi:softmax\"), (\"eval_metric\", \"merror\"), (\"alpha\", 8), (\"lambda\", 2), (\"num_class\", 10)]\n","n_rounds = 400\n","early_stopping = 50\n","    \n","d_train = xgb.DMatrix(X_train, label=y_train)\n","d_val = xgb.DMatrix(X_valid, label=y_valid)\n","eval_list = [(d_train, \"train\"), (d_val, \"validation\")]\n","bst = xgb.train(param_list, d_train, n_rounds, evals=eval_list, early_stopping_rounds=early_stopping, verbose_eval=True)\n","```\n","\n","**Obtained an accuracy of 0.8700 in kaggle**\n","\n","**sample code depth = 10 and n_classes = 10**\n","```\n","param_list = [(\"eta\", 0.08),  (\"max_depth\",10),(\"subsample\", 0.8), (\"colsample_bytree\", 0.8), (\"objective\", \"multi:softmax\"), (\"eval_metric\", \"merror\"), (\"alpha\", 8), (\"lambda\", 2), (\"num_class\", 10)]\n","n_rounds = 400\n","early_stopping = 50\n","    \n","d_train = xgb.DMatrix(X_train, label=y_train)\n","d_val = xgb.DMatrix(X_valid, label=y_valid)\n","eval_list = [(d_train, \"train\"), (d_val, \"validation\")]\n","bst = xgb.train(param_list, d_train, n_rounds, evals=eval_list, early_stopping_rounds=early_stopping, verbose_eval=True)\n","```\n","\n","**Obtained an accuracy of 0.88700 in kaggle**\n","\n","**time taken for training XGBoost with 50 PCA components is 906.3 sec ,n_rounds = 400,early_stopping = 50** \n","\n","**time taken for predicting with XGBoost trained with 50 PCA components 2.468 sec,n_rounds = 400,early_stopping = 50**\n","\n","\n","\n","**sample code depth = 10 and n_classes = 10 and 35 PCA components**\n","\n","\n","\n","```\n","param_list = [(\"eta\", 0.08),  (\"max_depth\",10),(\"subsample\", 0.8), (\"colsample_bytree\", 0.8), (\"objective\", \"multi:softmax\"), (\"eval_metric\", \"merror\"), (\"alpha\", 8), (\"lambda\", 2), (\"num_class\", 10)]\n","n_rounds = 400\n","early_stopping = 50\n","    \n","d_train = xgb.DMatrix(data_pca_subset[:,0:35], label=labels)\n","eval_list = [(d_train, \"train\")]\n","time_start = time.time()\n","bst = xgb.train(param_list, d_train, n_rounds, evals=eval_list, early_stopping_rounds=early_stopping,verbose_eval=True)\n","time_end = time.time()\n","```\n","\n","\n","**Obtained an accuracy of 0.89360 in kaggle**\n","\n","**time taken for training XGBoost with 35 PCA components is 602.01569843292236328 ,n_rounds = 400,early_stopping = 50**\n","\n","**time taken for predicting with XGBoost trained with 25 PCA components 2.6807 sec ,n_rounds = 400,early_stopping = 50** \n"]},{"cell_type":"markdown","metadata":{"id":"En1Rxk0Xjrm3","colab_type":"text"},"source":["# **Metrics**\n","\n","**Now lets look at some of the Metrics observed when training and testing for Random Forest models:**\n","\n","![alt text](https://drive.google.com/uc?id=1nW_k1FvHB4HnVhZ7sQx0O7uR-PncWGsd)\n","\n","\n","\n","*   We can see that mean validation score is highest for parameters max_depth = 20 and number of estimators = 200.\n","*   If we observe the highest F1 Score and lowest Log Loss values are associated with max_depth = 20 and number of estimators = 150.\n","*   If we have considered only mean validation score to finalise the parameters , we would have choosen max_depth = 20 and number of estimators = 200 which has more Log loss nad less F1 score compared to above prameters results.\n","*   So splitting training data into train and test sets and checking the model on test samples with metrics like Log Loss,F1 score,confusion matrix help in undersatnding how the model is performing.\n","*   Taking into consideration only mean validation accuracy and selecting model might result in ending with a overfit model.\n","\n","\n","***So the selected parameters were max_depth = 20 and n\n","_estimators = 150.***\n","\n","**sample code used**\n","\n","\n","\n","```\n","for x in depth :\n","    for y in trees:\n","        clf=RandomForestClassifier(max_depth=x, random_state=42,n_estimators=y)\n","        scores=cross_val_score(clf, train_x, train_y, cv=10)\n","        clf.fit(train_x, train_y)\n","        print(\"Mean Accuray score when depth={0} and num of tress={1} is : {2}\".format(x,y,scores.mean()))\n","        predictions = clf.predict(test_x)\n","        f1_s = f1_score(test_y, predictions,average='macro')\n","        f1score.append(f1_s)\n","        print(\"F1 score when depth = {0} and trees = {1} is {2}\".format(x,y,f1_s))\n","        rfcpred = clf.predict_proba(test_x)\n","        lg_loss = log_loss(test_y, rfcpred) \n","        logloss.append(\"Logarithm loss when depth =\"+ str(x) + \" and trees = \" + str(y) + \" is : \" + str(lg_loss))\n","        print(\"Logarithm loss when depth = {0} and trees = {1} is {2}\".format(x,y,lg_loss))\n","        print(confusion_matrix(test_y,predictions))\n","        deatils.append([x,y,scores.mean(),f1_s,lg_loss,confusion_matrix(test_y,predictions)])\n","```\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Th7bdlpkjrof","colab_type":"text"},"source":["### **Comparision:**\n","\n","**Lets compare the results of different models tried above:**\n","\n","![alt text](https://drive.google.com/uc?id=1FORcTfBFzb1ey08MC1uOCeJcl4_skGWH)\n","\n","**points to be noted**\n","\n","\n","*   We can notice that Randform forest is the quickiest in terms of prediction followed by XGBoost and SVM.Knn is very slow in predicting/classification.\n","*   As tree based classifiers build tress while training, prediction is very quick as they have to traverse through the tree.It explains the speed of Random Forests and XGBoost in prediction.\n","*   The more PCA components we use ,the more model is getting overfit.So using only required number of PCA components is a key in model tuning in all aspects. \n"]},{"cell_type":"markdown","metadata":{"id":"4N5blb1wjrqg","colab_type":"text"},"source":["# **Highest Kaggle Score recieved using Classic ML models is 89.76%**\n","\n","**Team Name : The Beard guy**"]},{"cell_type":"code","metadata":{"id":"cVEGvnATuBGw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}